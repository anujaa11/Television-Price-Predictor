{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db806078",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fc0441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data from page 1 (Items scraped: 24)\n",
      "Scraped data from page 2 (Items scraped: 48)\n",
      "Scraped data from page 3 (Items scraped: 72)\n",
      "Scraped data from page 4 (Items scraped: 96)\n",
      "Scraped data from page 5 (Items scraped: 120)\n",
      "Scraped data from page 6 (Items scraped: 144)\n",
      "Scraped data from page 7 (Items scraped: 168)\n",
      "Scraped data from page 8 (Items scraped: 192)\n",
      "Scraped data from page 9 (Items scraped: 216)\n",
      "Scraped data from page 10 (Items scraped: 216)\n",
      "Scraped data from page 11 (Items scraped: 240)\n",
      "Scraped data from page 12 (Items scraped: 264)\n",
      "Scraped data from page 13 (Items scraped: 288)\n",
      "Scraped data from page 14 (Items scraped: 312)\n",
      "Scraped data from page 15 (Items scraped: 336)\n",
      "Scraped data from page 16 (Items scraped: 360)\n",
      "Scraped data from page 17 (Items scraped: 384)\n",
      "Scraped data from page 18 (Items scraped: 384)\n",
      "Scraped data from page 19 (Items scraped: 408)\n",
      "Scraped data from page 20 (Items scraped: 432)\n",
      "Scraped data from page 21 (Items scraped: 456)\n",
      "Scraped data from page 22 (Items scraped: 456)\n",
      "Scraped data from page 23 (Items scraped: 480)\n",
      "Scraped data from page 24 (Items scraped: 504)\n",
      "Scraped data from page 25 (Items scraped: 528)\n",
      "Scraped data from page 26 (Items scraped: 552)\n",
      "Scraped data from page 27 (Items scraped: 576)\n",
      "Scraped data from page 28 (Items scraped: 600)\n",
      "Scraped data from page 29 (Items scraped: 624)\n",
      "Scraped data from page 30 (Items scraped: 624)\n",
      "Scraped data from page 31 (Items scraped: 648)\n",
      "Scraped data from page 32 (Items scraped: 648)\n",
      "Scraped data from page 33 (Items scraped: 672)\n",
      "Scraped data from page 34 (Items scraped: 696)\n",
      "Scraped data from page 35 (Items scraped: 720)\n",
      "Scraped data from page 36 (Items scraped: 744)\n",
      "Scraped data from page 37 (Items scraped: 768)\n",
      "Scraped data from page 38 (Items scraped: 792)\n",
      "Scraped data from page 39 (Items scraped: 816)\n",
      "Scraped data from page 40 (Items scraped: 840)\n",
      "Scraped data from page 41 (Items scraped: 864)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find('span', class_='B_NuCI')\n",
    "        title_value = title.text\n",
    "        title_string = title_value.strip()\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Deal Price\n",
    "def get_deal_price(soup):\n",
    "    deal_price = soup.find(\"div\", class_='_30jeq3')\n",
    "    deal_price_value = deal_price.text.strip() if deal_price else \"\"\n",
    "    return deal_price_value\n",
    "\n",
    "# Function to extract Screen Type\n",
    "def get_screen_type(soup):\n",
    "    screen_type_td = soup.find(\"td\", text=\"Screen Type\")\n",
    "    if screen_type_td:\n",
    "        li_element = screen_type_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract HDMI count\n",
    "def get_hdmi_count(soup):\n",
    "    hdmi_td = soup.find(\"td\", text=\"HDMI\")\n",
    "    if hdmi_td:\n",
    "        li_element = hdmi_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract USB count\n",
    "def get_usb_count(soup):\n",
    "    usb_td = soup.find(\"td\", text=\"USB\")\n",
    "    if usb_td:\n",
    "        li_element = usb_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract Operating System\n",
    "def get_os(soup):\n",
    "    os_td = soup.find(\"td\", text=\"Operating System\")\n",
    "    if os_td:\n",
    "        li_element = os_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract Smart TV information\n",
    "def get_smart_tv(soup):\n",
    "    smart_tv_td = soup.find(\"td\", text=\"Smart Tv\")\n",
    "    if smart_tv_td:\n",
    "        li_element = smart_tv_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract Display Size information\n",
    "def get_display_size(soup):\n",
    "    display_size_td = soup.find(\"td\", text=\"Display Size\")\n",
    "    if display_size_td:\n",
    "        li_element = display_size_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract HD Technology & Resolution\n",
    "def get_hd_technology_resolution(soup):\n",
    "    hd_technology_resolution_td = soup.find(\"td\", text=\"HD Technology & Resolution\")\n",
    "    if hd_technology_resolution_td:\n",
    "        ul_element = hd_technology_resolution_td.find_next(\"ul\")\n",
    "        li_element = ul_element.find(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract Launch Year\n",
    "def get_launch_year(soup):\n",
    "    launch_year_td = soup.find(\"td\", text=\"Launch Year\")\n",
    "    if launch_year_td:\n",
    "        li_element = launch_year_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "# Function to extract Model Name\n",
    "def get_model_name(soup):\n",
    "    model_name_td = soup.find(\"td\", text=\"Model Name\")\n",
    "    if model_name_td:\n",
    "        li_element = model_name_td.find_next(\"li\", class_=\"_21lJbe\")\n",
    "        if li_element:\n",
    "            return li_element.text\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d = {\n",
    "        \"title\": [],\n",
    "        \"deal_price\": [],\n",
    "        \"screen_type\": [],\n",
    "        \"hdmi_count\": [],\n",
    "        \"usb_count\": [],\n",
    "        \"os\": [],\n",
    "        \"smart_tv\": [],\n",
    "        \"display_size\": [],\n",
    "        \"hd_technology_resolution\": [],\n",
    "        \"launch_year\": [],\n",
    "        \"model_name\": [],  # Add the model_name key\n",
    "    }\n",
    "\n",
    "    # Define the number of pages to scrape\n",
    "    num_pages = 41\n",
    "    items_scraped = 0  # Initialize a counter for items scraped\n",
    "\n",
    "    # Iterate through the pages\n",
    "    for page_number in range(1, num_pages + 1):\n",
    "        URL = f\"https://www.flipkart.com/search?q=TV&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={page_number}\"\n",
    "\n",
    "        retry = 3  # Number of retries in case of request failure\n",
    "        while retry > 0:\n",
    "            try:\n",
    "                r = requests.get(URL)\n",
    "                soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "                links = soup.find_all(\"a\", class_='_1fQZEK')\n",
    "\n",
    "                for link in links:\n",
    "                    new_webpage = requests.get(\"https://www.flipkart.com\" + link.get('href'))\n",
    "                    new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "                    title = get_title(new_soup)\n",
    "                    deal_price = get_deal_price(new_soup)\n",
    "                    screen_type = get_screen_type(new_soup)\n",
    "                    hdmi_count = get_hdmi_count(new_soup)\n",
    "                    usb_count = get_usb_count(new_soup)\n",
    "                    os = get_os(new_soup)\n",
    "                    smart_tv = get_smart_tv(new_soup)\n",
    "                    display_size = get_display_size(new_soup)\n",
    "                    hd_technology_resolution = get_hd_technology_resolution(new_soup)\n",
    "                    launch_year = get_launch_year(new_soup)\n",
    "                    model_name = get_model_name(new_soup)  # Get the model name\n",
    "\n",
    "                    # Add the extracted data to the dictionary\n",
    "                    d['title'].append(title)\n",
    "                    d['deal_price'].append(deal_price)\n",
    "                    d['screen_type'].append(screen_type)\n",
    "                    d['hdmi_count'].append(hdmi_count)\n",
    "                    d['usb_count'].append(usb_count)\n",
    "                    d['os'].append(os)\n",
    "                    d['smart_tv'].append(smart_tv)\n",
    "                    d['display_size'].append(display_size)\n",
    "                    d['hd_technology_resolution'].append(hd_technology_resolution)\n",
    "                    d['launch_year'].append(launch_year)\n",
    "                    d['model_name'].append(model_name)  # Add the model name\n",
    "\n",
    "                    items_scraped += 1\n",
    "\n",
    "                # Print the current page number for tracking progress\n",
    "                print(f\"Scraped data from page {page_number} (Items scraped: {items_scraped})\")\n",
    "\n",
    "                # Break out of retry loop if the page was successfully scraped\n",
    "                break\n",
    "            except Exception as e:\n",
    "                # Handle request errors\n",
    "                print(f\"Error while scraping page {page_number}: {str(e)}\")\n",
    "                retry -= 1\n",
    "                if retry == 0:\n",
    "                    print(f\"Failed to scrape page {page_number}.\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Retrying page {page_number}...\")\n",
    "                    time.sleep(5)  # Wait for a few seconds before retrying\n",
    "\n",
    "    df = pd.DataFrame.from_dict(d)\n",
    "\n",
    "    df.to_csv(\"Dataset.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11b3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f7003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
